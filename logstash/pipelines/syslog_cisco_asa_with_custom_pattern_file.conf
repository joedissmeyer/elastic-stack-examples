# Example pipeline for receiving syslog data from Cisco ASA firewalls and appliances.
# The additional custom patterns file is called in the grok filter as shown below.
# Special notes:
# --- The default syslog listening port is 514. But in Linux ports within the 0-1024 range require the logstash app to run as root.
# --- Recommend to change the listening port to something higher instead of running logstash as root.
# --- Do not forget to also reference the elasticsearch index template for cisco_asa. Important notes are there for long term index management!

input {
  # Define the port to listen for syslog data.
  udp {
    id => "udp_cisco_firewall_syslog"
    port => 5514
  }
}

filter {
  # Split the syslog part and Cisco tag out of the message
  grok {
    match => ["message", "%{CISCO_TAGGED_SYSLOG} %{GREEDYDATA:cisco_message}"]
  }

  # Parse the syslog severity and facility
  syslog_pri { }

  # Parse the date from the "timestamp" field to the "@timestamp" field
  date {
    match => ["timestamp",
      "MMM dd HH:mm:ss",
      "MMM  d HH:mm:ss",
      "MMM dd yyyy HH:mm:ss",
      "MMM  d yyyy HH:mm:ss"
    ]
    timezone => "America/New_York"
  }

  # Clean up redundant fields if parsing was successful
  if "_grokparsefailure" not in [tags] {
    mutate {
      rename => ["cisco_message", "message"]
      remove_field => ["timestamp"]
    }
  }

  # Extract fields from the each of the detailed message types
  # Most of the patterns provided below are included in Logstash since 1.2.0. All others are defined in the custom patterns file.
  grok {
    # Include the custom patterns in this grok filter.
    patterns_dir => [ "/etc/logstash/patterns/cisco_asa" ]
    match => [
      "message", "%{CISCOFW106001}",
      "message", "%{CISCOFW106006_106007_106010}",
      "message", "%{CISCOFW106014}",
      "message", "%{CISCOFW106015}",
      "message", "%{CISCOFW106021}",
      "message", "%{CISCOFW106023}",
      "message", "%{CISCOFW106100}",
      "message", "%{CISCOFW110002}",
      "message", "%{CISCOFW302010}",
      "message", "%{CISCOFW302013_302014_302015_302016}",
      "message", "%{CISCOFW302020_302021}",
      "message", "%{CISCOFW305011}",
      "message", "%{CISCOFW313001_313004_313008}",
      "message", "%{CISCOFW313005}",
      "message", "%{CISCOFW402117}",
      "message", "%{CISCOFW402119}",
      "message", "%{CISCOFW419001}",
      "message", "%{CISCOFW419002}",
      "message", "%{CISCOFW500004}",
      "message", "%{CISCOFW602303_602304}",
      "message", "%{CISCOFW710001_710002_710003_710005_710006}",
      "message", "%{CISCOFW713172}",
      "message", "%{CISCOFW733100}",
      "message", "%{CISCOFW713050}",
      "message", "%{CISCOFW713259}",
      "message", "%{CISCOFW113019}",
      "message", "%{CISCOFW713119}",
      "message", "%{CISCOFW713120}",
      "message", "%{CISCOFW713049}",
      "message", "%{CISCOFW713076}",
      "message", "%{CISCOFW713130}",
      "message", "%{CISCOFW722012}",
      "message", "%{CISCOFW722037}",
      "message", "%{CISCOFW722041}",
      "message", "%{CISCOFW722051}",
      "message", "%{CISCOFW722010}",
      "message", "%{CISCOFW722028}",
      "message", "%{CISCOFW722032}",
      "message", "%{CISCOFW722033}",
      "message", "%{CISCOFW722034}",
      "message", "%{CISCOFW737003}",
      "message", "%{CISCOFW737034}"
    ]
  }

  geoip { source => "src_ip" }

}

output {
  elasticsearch {
    hosts => ["es_data_node1:9200","es_data_node2:9200","es_data_node3:9200","es_data_node4:9200"]
    manage_template => false
    template_name => "cisco_asa"
    index => "cisco_asa-%{+YYYY.MM.dd}"
    user => "logstash_user"
    password => "logstash_password"
  }
}